# Writing Assistant - Integration Guide

## Overview
This guide shows how to build a production-grade writing assistant similar to Grammarly, including grammar checking, style analysis, and plagiarism detection.

## Architecture

### Frontend (React/HTML)
- Real-time text editing with syntax highlighting
- Issue visualization and inline suggestions
- Statistics dashboard
- Responsive design

### Backend (Python/Node.js)
- NLP processing pipeline
- Grammar and style checking
- Plagiarism detection
- API endpoints for analysis

## Advanced Features Implementation

### 1. Grammar Checking with LanguageTool

LanguageTool is an open-source grammar checker supporting 20+ languages.

```python
import language_tool_python

# Initialize
tool = language_tool_python.LanguageTool('en-US')

def check_grammar(text):
    matches = tool.check(text)
    issues = []
    
    for match in matches:
        issues.append({
            'type': 'error' if match.ruleIssueType == 'misspelling' else 'warning',
            'position': match.offset,
            'length': match.errorLength,
            'text': text[match.offset:match.offset + match.errorLength],
            'suggestion': match.replacements[:3] if match.replacements else [],
            'message': match.message,
            'rule': match.ruleId
        })
    
    return issues

# Usage
text = "I has a dream"
issues = check_grammar(text)
```

### 2. Advanced NLP with spaCy

spaCy provides deep linguistic analysis.

```python
import spacy

nlp = spacy.load('en_core_web_sm')

def analyze_text(text):
    doc = nlp(text)
    
    analysis = {
        'entities': [(ent.text, ent.label_) for ent in doc.ents],
        'pos_tags': [(token.text, token.pos_) for token in doc],
        'dependencies': [(token.text, token.dep_, token.head.text) for token in doc],
        'sentences': [sent.text for sent in doc.sents]
    }
    
    # Detect passive voice
    passive_sentences = []
    for sent in doc.sents:
        for token in sent:
            if token.dep_ == 'nsubjpass':
                passive_sentences.append(sent.text)
                break
    
    analysis['passive_voice'] = passive_sentences
    
    return analysis
```

### 3. Semantic Similarity for Plagiarism Detection

Use sentence transformers to detect paraphrased content.

```python
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

def detect_similarity(text, comparison_texts):
    """
    Detect semantic similarity between text and a corpus
    """
    # Encode the query text
    query_embedding = model.encode(text)
    
    # Encode comparison texts
    corpus_embeddings = model.encode(comparison_texts)
    
    # Calculate cosine similarity
    similarities = []
    for i, emb in enumerate(corpus_embeddings):
        similarity = np.dot(query_embedding, emb) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(emb)
        )
        similarities.append({
            'text': comparison_texts[i],
            'similarity': float(similarity),
            'match': similarity > 0.75  # Threshold
        })
    
    return sorted(similarities, key=lambda x: x['similarity'], reverse=True)

# Usage
my_text = "The quick brown fox jumps over the lazy dog"
corpus = [
    "A fast brown fox leaps over a sleepy dog",
    "The weather is nice today",
    "Machine learning is fascinating"
]

results = detect_similarity(my_text, corpus)
# First result will show high similarity (paraphrase detected)
```

### 4. Web-Based Plagiarism Checking

Integrate with search APIs to check against online content.

```python
import requests
from bs4 import BeautifulSoup

def check_online_plagiarism(text, api_key):
    """
    Search for text online using Google Custom Search API
    """
    # Extract distinctive phrases (5-8 words)
    sentences = text.split('.')
    phrases = []
    
    for sentence in sentences:
        words = sentence.strip().split()
        if len(words) >= 5:
            # Take first 7 words as search query
            phrase = ' '.join(words[:7])
            phrases.append(phrase)
    
    results = []
    
    for phrase in phrases[:5]:  # Limit API calls
        # Google Custom Search API
        url = 'https://www.googleapis.com/customsearch/v1'
        params = {
            'key': api_key,
            'cx': 'your_search_engine_id',
            'q': f'"{phrase}"',  # Exact phrase search
            'num': 5
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        if 'items' in data:
            for item in data['items']:
                results.append({
                    'phrase': phrase,
                    'url': item['link'],
                    'title': item['title'],
                    'snippet': item['snippet']
                })
    
    return results
```

### 5. Machine Learning Style Classifier

Train a model to detect writing style issues.

```python
from transformers import pipeline

# Use pre-trained model for text classification
classifier = pipeline('text-classification', 
                     model='distilbert-base-uncased-finetuned-sst-2-english')

def analyze_tone(text):
    """
    Analyze emotional tone and formality
    """
    result = classifier(text)[0]
    
    # Custom analysis
    formal_indicators = ['furthermore', 'consequently', 'therefore', 'hence']
    informal_indicators = ['gonna', 'wanna', 'kinda', 'sorta', 'yeah']
    
    text_lower = text.lower()
    
    formality_score = (
        sum(1 for word in formal_indicators if word in text_lower) * 10 -
        sum(1 for word in informal_indicators if word in text_lower) * 10
    )
    
    return {
        'sentiment': result['label'],
        'confidence': result['score'],
        'formality': 'formal' if formality_score > 0 else 'informal',
        'formality_score': formality_score
    }
```

### 6. Real-time API with FastAPI

Create a REST API for your writing assistant.

```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="Writing Assistant API")

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AnalysisRequest(BaseModel):
    text: str
    check_plagiarism: bool = True
    check_style: bool = True

class Issue(BaseModel):
    type: str
    category: str
    position: int
    length: int
    text: str
    suggestion: str
    confidence: float = 1.0

class AnalysisResponse(BaseModel):
    issues: List[Issue]
    statistics: dict
    score: int
    readability: dict

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_text(request: AnalysisRequest):
    try:
        assistant = WritingAssistant()
        result = assistant.analyze(request.text)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# Run with: uvicorn api:app --reload
```

## Integration with Claude API

For the most advanced analysis, integrate with Claude:

```python
import anthropic

client = anthropic.Anthropic(api_key="your_api_key")

def advanced_analysis_with_claude(text):
    """
    Use Claude for sophisticated writing analysis
    """
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"""Analyze this text for:
1. Grammar and spelling errors
2. Style improvements
3. Clarity and conciseness
4. Tone and formality
5. Structure and organization

Text: {text}

Provide detailed feedback in JSON format with specific suggestions."""
        }]
    )
    
    return message.content
```

## Production Considerations

### 1. Performance Optimization
- **Caching**: Cache analysis results for unchanged text
- **Async Processing**: Use background jobs for plagiarism checking
- **Debouncing**: Wait for user to stop typing before analysis
- **Incremental Analysis**: Only re-check changed portions

### 2. Scalability
- **Database**: Store user documents and analysis history
- **Queue System**: Use Redis/Celery for background tasks
- **CDN**: Serve static assets efficiently
- **Load Balancing**: Distribute API requests

### 3. Data Privacy
- **Encryption**: Encrypt documents at rest and in transit
- **User Consent**: Clear privacy policy for content processing
- **Data Retention**: Implement deletion policies
- **Anonymization**: Remove PII from training data

### 4. API Rate Limiting

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/analyze")
@limiter.limit("10/minute")
async def analyze_text(request: AnalysisRequest):
    # Analysis logic
    pass
```

## Third-Party Services

### Plagiarism APIs
1. **Copyscape** - Web-based plagiarism detection
2. **Turnitin** - Academic plagiarism checking
3. **Quetext** - Deep search plagiarism detection
4. **Grammarly API** - If building complementary tools

### Grammar APIs
1. **LanguageTool** - Open source, self-hostable
2. **Ginger Software API** - Commercial option
3. **ProWritingAid API** - Style and grammar
4. **After the Deadline** - Free, limited features

### Example Copyscape Integration

```python
import requests

def check_copyscape(text, api_key, username):
    """
    Check text against Copyscape database
    """
    url = 'https://www.copyscape.com/api/'
    
    params = {
        'u': username,
        'k': api_key,
        'o': 'csearch',
        't': text,
        'f': 'json'
    }
    
    response = requests.post(url, data=params)
    data = response.json()
    
    if 'result' in data:
        matches = []
        for result in data['result']:
            matches.append({
                'url': result['url'],
                'title': result['title'],
                'percentage': result['percentmatched'],
                'words_matched': result['wordsmatched']
            })
        return matches
    
    return []
```

## Frontend Integration Example

```javascript
// Real-time analysis with debouncing
let analysisTimeout;
const editor = document.getElementById('editor');

editor.addEventListener('input', () => {
    clearTimeout(analysisTimeout);
    
    analysisTimeout = setTimeout(async () => {
        const text = editor.value;
        
        if (text.length < 10) return;
        
        try {
            const response = await fetch('http://localhost:8000/analyze', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    text: text,
                    check_plagiarism: true,
                    check_style: true
                })
            });
            
            const result = await response.json();
            displayIssues(result.issues);
            updateScore(result.score);
            
        } catch (error) {
            console.error('Analysis failed:', error);
        }
    }, 1000); // Wait 1 second after typing stops
});

function displayIssues(issues) {
    // Highlight issues in the editor
    // Show suggestions in sidebar
    // Update issue count
}
```

## Deployment

### Docker Setup

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Download spaCy model
RUN python -m spacy download en_core_web_sm

COPY . .

EXPOSE 8000

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Requirements.txt

```
fastapi==0.104.1
uvicorn[standard]==0.24.0
language-tool-python==2.7.1
spacy==3.7.2
sentence-transformers==2.2.2
transformers==4.35.2
textstat==0.7.3
nltk==3.8.1
beautifulsoup4==4.12.2
requests==2.31.0
pydantic==2.5.0
python-multipart==0.0.6
slowapi==0.1.9
```

## Testing

```python
import pytest
from writing_assistant import WritingAssistant

def test_grammar_detection():
    assistant = WritingAssistant()
    result = assistant.analyze("I has a problem")
    
    assert len(result['issues']) > 0
    assert any(issue['type'] == 'error' for issue in result['issues'])

def test_plagiarism_detection():
    assistant = WritingAssistant()
    result = assistant.analyze("To be or not to be")
    
    plagiarism_issues = [i for i in result['issues'] if i['type'] == 'plagiarism']
    assert len(plagiarism_issues) > 0

def test_score_calculation():
    assistant = WritingAssistant()
    
    # Perfect text should score high
    result = assistant.analyze("This is a well-written sentence.")
    assert result['score'] > 80
    
    # Text with errors should score lower
    result = assistant.analyze("this  is bad grammer and spelling")
    assert result['score'] < 50
```

## Next Steps

1. **User Authentication**: Add user accounts and document storage
2. **Document History**: Track changes and improvements over time
3. **Custom Dictionaries**: Allow users to add terms
4. **Team Features**: Shared style guides and collaboration
5. **Browser Extension**: Check text in any input field
6. **Mobile Apps**: Native iOS/Android applications
7. **API Marketplace**: Offer API access to developers

## Resources

- [LanguageTool Documentation](https://languagetool.org/dev)
- [spaCy Documentation](https://spacy.io/usage)
- [Sentence Transformers](https://www.sbert.net/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Anthropic API Documentation](https://docs.anthropic.com/)

