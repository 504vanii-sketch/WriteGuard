"""
Accuracy Testing Framework for Writing Assistant
================================================

This module provides comprehensive testing to ensure high accuracy in:
1. Grammar checking
2. Style suggestions
3. Plagiarism detection
4. Readability metrics
"""

import json
from typing import List, Dict, Tuple
from dataclasses import dataclass
import statistics

@dataclass
class TestCase:
    """Represents a single test case"""
    text: str
    expected_issues: List[Dict]
    category: str
    description: str

@dataclass
class TestResult:
    """Results of a test case"""
    passed: bool
    true_positives: int
    false_positives: int
    false_negatives: int
    precision: float
    recall: float
    f1_score: float
    details: str

class AccuracyTester:
    """Test and validate writing assistant accuracy"""
    
    def __init__(self, assistant):
        self.assistant = assistant
        self.test_cases = self._load_test_cases()
        
    def _load_test_cases(self) -> List[TestCase]:
        """Load comprehensive test cases"""
        
        # Grammar test cases
        grammar_tests = [
            TestCase(
                text="I has a dream",
                expected_issues=[
                    {'type': 'error', 'text': 'has', 'category': 'Grammar'}
                ],
                category='grammar',
                description='Subject-verb agreement error'
            ),
            TestCase(
                text="She don't like pizza",
                expected_issues=[
                    {'type': 'error', 'text': "don't", 'category': 'Grammar'}
                ],
                category='grammar',
                description='Subject-verb agreement with contraction'
            ),
            TestCase(
                text="Their going to the store",
                expected_issues=[
                    {'type': 'error', 'text': 'Their', 'category': 'Grammar'}
                ],
                category='grammar',
                description='Their/there/they\'re confusion'
            ),
            TestCase(
                text="Its a beautiful day",
                expected_issues=[
                    {'type': 'error', 'text': 'Its', 'category': 'Grammar'}
                ],
                category='grammar',
                description='Its/it\'s confusion'
            ),
            TestCase(
                text="I should of gone there",
                expected_issues=[
                    {'type': 'error', 'text': 'should of', 'category': 'Grammar'}
                ],
                category='grammar',
                description='Should of instead of should have'
            ),
            TestCase(
                text="The dog dog ran fast",
                expected_issues=[
                    {'type': 'error', 'text': 'dog dog', 'category': 'Grammar'}
                ],
                category='grammar',
                description='Duplicate word detection'
            ),
            TestCase(
                text="This is a apple",
                expected_issues=[
                    {'type': 'error', 'text': 'a apple', 'category': 'Grammar'}
                ],
                category='grammar',
                description='Article error (a vs an)'
            ),
            TestCase(
                text="He walk to school",
                expected_issues=[
                    {'type': 'error', 'text': 'walk', 'category': 'Grammar'}
                ],
                category='grammar',
                description='Missing verb conjugation'
            ),
        ]
        
        # Style test cases
        style_tests = [
            TestCase(
                text="This is very very important",
                expected_issues=[
                    {'type': 'style', 'category': 'Style'}
                ],
                category='style',
                description='Weak intensifier detection'
            ),
            TestCase(
                text="I just wanted to say that basically it's really good",
                expected_issues=[
                    {'type': 'style', 'category': 'Style'}
                ],
                category='style',
                description='Multiple weak words'
            ),
            TestCase(
                text="The report was written by the team",
                expected_issues=[
                    {'type': 'style', 'category': 'Style'}
                ],
                category='style',
                description='Passive voice detection'
            ),
            TestCase(
                text="In order to improve performance",
                expected_issues=[
                    {'type': 'style', 'text': 'in order to', 'category': 'Style'}
                ],
                category='style',
                description='Wordy phrase detection'
            ),
        ]
        
        # Plagiarism test cases
        plagiarism_tests = [
            TestCase(
                text="To be or not to be, that is the question",
                expected_issues=[
                    {'type': 'plagiarism', 'category': 'Plagiarism'}
                ],
                category='plagiarism',
                description='Famous quote detection'
            ),
            TestCase(
                text="Climate change is one of the most pressing issues of our time",
                expected_issues=[
                    {'type': 'plagiarism', 'category': 'Plagiarism'}
                ],
                category='plagiarism',
                description='Common academic phrase'
            ),
        ]
        
        # Clean text (no errors) - important for false positive testing
        clean_tests = [
            TestCase(
                text="The quick brown fox jumps over the lazy dog.",
                expected_issues=[],
                category='clean',
                description='Perfect sentence with no errors'
            ),
            TestCase(
                text="She walks to school every morning. The weather is usually pleasant.",
                expected_issues=[],
                category='clean',
                description='Clean text should not trigger false positives'
            ),
            TestCase(
                text="Machine learning algorithms can identify patterns in large datasets.",
                expected_issues=[],
                category='clean',
                description='Technical writing without errors'
            ),
        ]
        
        return grammar_tests + style_tests + plagiarism_tests + clean_tests
    
    def _match_issue(self, found_issue: Dict, expected_issue: Dict) -> bool:
        """Check if a found issue matches an expected issue"""
        type_match = found_issue['type'] == expected_issue.get('type', found_issue['type'])
        category_match = found_issue['category'] == expected_issue.get('category', found_issue['category'])
        
        # If expected issue specifies text, check for overlap
        if 'text' in expected_issue:
            text_overlap = expected_issue['text'].lower() in found_issue['text'].lower()
            return type_match and category_match and text_overlap
        
        return type_match and category_match
    
    def test_single_case(self, test_case: TestCase) -> TestResult:
        """Test a single case and calculate accuracy metrics"""
        
        # Run analysis
        result = self.assistant.analyze(test_case.text)
        found_issues = result['issues']
        
        # Calculate metrics
        true_positives = 0
        false_positives = 0
        false_negatives = 0
        
        matched_found = set()
        matched_expected = set()
        
        # Find true positives
        for i, found in enumerate(found_issues):
            for j, expected in enumerate(test_case.expected_issues):
                if j not in matched_expected and self._match_issue(found, expected):
                    true_positives += 1
                    matched_found.add(i)
                    matched_expected.add(j)
                    break
        
        # Calculate false positives (found but not expected)
        false_positives = len(found_issues) - true_positives
        
        # Calculate false negatives (expected but not found)
        false_negatives = len(test_case.expected_issues) - true_positives
        
        # Calculate precision and recall
        precision = true_positives / max(1, true_positives + false_positives)
        recall = true_positives / max(1, true_positives + false_negatives)
        f1_score = 2 * (precision * recall) / max(0.001, precision + recall)
        
        # Determine if test passed
        passed = (false_positives == 0 and false_negatives == 0)
        
        # Generate details
        details = f"TP: {true_positives}, FP: {false_positives}, FN: {false_negatives}"
        if false_positives > 0:
            fp_issues = [f for i, f in enumerate(found_issues) if i not in matched_found]
            details += f"\nFalse Positives: {fp_issues}"
        if false_negatives > 0:
            fn_issues = [e for i, e in enumerate(test_case.expected_issues) if i not in matched_expected]
            details += f"\nFalse Negatives: {fn_issues}"
        
        return TestResult(
            passed=passed,
            true_positives=true_positives,
            false_positives=false_positives,
            false_negatives=false_negatives,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            details=details
        )
    
    def run_all_tests(self) -> Dict:
        """Run all test cases and generate comprehensive report"""
        
        results_by_category = {}
        all_results = []
        
        for test_case in self.test_cases:
            result = self.test_single_case(test_case)
            all_results.append((test_case, result))
            
            if test_case.category not in results_by_category:
                results_by_category[test_case.category] = []
            results_by_category[test_case.category].append(result)
        
        # Calculate overall metrics
        overall_metrics = self._calculate_metrics(all_results)
        
        # Calculate category-specific metrics
        category_metrics = {}
        for category, results in results_by_category.items():
            category_results = [(tc, r) for tc, r in all_results if tc.category == category]
            category_metrics[category] = self._calculate_metrics(category_results)
        
        return {
            'overall': overall_metrics,
            'by_category': category_metrics,
            'detailed_results': all_results,
            'total_tests': len(self.test_cases),
            'passed_tests': sum(1 for _, r in all_results if r.passed)
        }
    
    def _calculate_metrics(self, results: List[Tuple[TestCase, TestResult]]) -> Dict:
        """Calculate aggregate metrics for a set of results"""
        
        if not results:
            return {}
        
        total_tp = sum(r.true_positives for _, r in results)
        total_fp = sum(r.false_positives for _, r in results)
        total_fn = sum(r.false_negatives for _, r in results)
        
        precision = total_tp / max(1, total_tp + total_fp)
        recall = total_tp / max(1, total_tp + total_fn)
        f1_score = 2 * (precision * recall) / max(0.001, precision + recall)
        
        accuracy = sum(1 for _, r in results if r.passed) / len(results)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'true_positives': total_tp,
            'false_positives': total_fp,
            'false_negatives': total_fn
        }
    
    def print_report(self, report: Dict):
        """Print a formatted test report"""
        
        print("\n" + "="*70)
        print("WRITING ASSISTANT ACCURACY REPORT")
        print("="*70)
        
        print(f"\nTests Run: {report['total_tests']}")
        print(f"Tests Passed: {report['passed_tests']}")
        print(f"Pass Rate: {report['passed_tests']/report['total_tests']*100:.1f}%")
        
        print("\n" + "-"*70)
        print("OVERALL METRICS")
        print("-"*70)
        
        overall = report['overall']
        print(f"Accuracy:  {overall['accuracy']*100:.1f}%")
        print(f"Precision: {overall['precision']*100:.1f}%")
        print(f"Recall:    {overall['recall']*100:.1f}%")
        print(f"F1 Score:  {overall['f1_score']*100:.1f}%")
        
        print("\n" + "-"*70)
        print("METRICS BY CATEGORY")
        print("-"*70)
        
        for category, metrics in report['by_category'].items():
            print(f"\n{category.upper()}:")
            print(f"  Accuracy:  {metrics['accuracy']*100:.1f}%")
            print(f"  Precision: {metrics['precision']*100:.1f}%")
            print(f"  Recall:    {metrics['recall']*100:.1f}%")
            print(f"  F1 Score:  {metrics['f1_score']*100:.1f}%")
        
        print("\n" + "-"*70)
        print("FAILED TESTS")
        print("-"*70)
        
        failed_count = 0
        for test_case, result in report['detailed_results']:
            if not result.passed:
                failed_count += 1
                print(f"\n{failed_count}. {test_case.description}")
                print(f"   Text: \"{test_case.text}\"")
                print(f"   {result.details}")
        
        if failed_count == 0:
            print("\nNo failed tests! 🎉")
        
        print("\n" + "="*70)


class BenchmarkComparison:
    """Compare against industry benchmarks and other tools"""
    
    def __init__(self):
        # Industry benchmarks (approximate values from research)
        self.benchmarks = {
            'grammar': {
                'grammarly': {'precision': 0.92, 'recall': 0.88, 'f1': 0.90},
                'languagetool': {'precision': 0.85, 'recall': 0.82, 'f1': 0.83},
                'word_processor': {'precision': 0.75, 'recall': 0.70, 'f1': 0.72}
            },
            'style': {
                'grammarly': {'precision': 0.78, 'recall': 0.75, 'f1': 0.76},
                'prowritingaid': {'precision': 0.80, 'recall': 0.77, 'f1': 0.78}
            },
            'plagiarism': {
                'turnitin': {'precision': 0.95, 'recall': 0.90, 'f1': 0.92},
                'copyscape': {'precision': 0.88, 'recall': 0.85, 'f1': 0.86}
            }
        }
    
    def compare(self, your_metrics: Dict) -> Dict:
        """Compare your metrics against benchmarks"""
        
        comparisons = {}
        
        for category, your_results in your_metrics.items():
            if category in self.benchmarks:
                comparisons[category] = {}
                
                for tool, benchmark in self.benchmarks[category].items():
                    difference = {
                        'precision_diff': your_results['precision'] - benchmark['precision'],
                        'recall_diff': your_results['recall'] - benchmark['recall'],
                        'f1_diff': your_results['f1_score'] - benchmark['f1']
                    }
                    
                    comparisons[category][tool] = {
                        'benchmark': benchmark,
                        'difference': difference,
                        'competitive': difference['f1_diff'] >= -0.05  # Within 5%
                    }
        
        return comparisons
    
    def print_comparison(self, comparisons: Dict):
        """Print benchmark comparison report"""
        
        print("\n" + "="*70)
        print("BENCHMARK COMPARISON")
        print("="*70)
        
        for category, tools in comparisons.items():
            print(f"\n{category.upper()}:")
            
            for tool, data in tools.items():
                print(f"\n  vs {tool}:")
                benchmark = data['benchmark']
                diff = data['difference']
                
                print(f"    Precision: {benchmark['precision']:.2f} (you: {diff['precision_diff']:+.2f})")
                print(f"    Recall:    {benchmark['recall']:.2f} (you: {diff['recall_diff']:+.2f})")
                print(f"    F1 Score:  {benchmark['f1']:.2f} (you: {diff['f1_diff']:+.2f})")
                
                if data['competitive']:
                    print(f"    Status: ✓ Competitive")
                else:
                    print(f"    Status: ✗ Needs improvement")


class ContinuousValidation:
    """System for ongoing accuracy monitoring"""
    
    def __init__(self):
        self.validation_log = []
    
    def collect_user_feedback(self, text: str, issue: Dict, user_agreed: bool):
        """Collect feedback on whether detection was accurate"""
        
        self.validation_log.append({
            'text': text,
            'issue': issue,
            'user_agreed': user_agreed,
            'timestamp': 'timestamp_here'
        })
    
    def calculate_user_agreement_rate(self) -> float:
        """Calculate percentage of times users agreed with suggestions"""
        
        if not self.validation_log:
            return 0.0
        
        agreements = sum(1 for entry in self.validation_log if entry['user_agreed'])
        return agreements / len(self.validation_log)
    
    def identify_problematic_patterns(self) -> List[Dict]:
        """Find rules that frequently get rejected by users"""
        
        issue_stats = {}
        
        for entry in self.validation_log:
            rule_id = entry['issue'].get('suggestion', 'unknown')
            
            if rule_id not in issue_stats:
                issue_stats[rule_id] = {'total': 0, 'agreed': 0}
            
            issue_stats[rule_id]['total'] += 1
            if entry['user_agreed']:
                issue_stats[rule_id]['agreed'] += 1
        
        # Find rules with low agreement rate
        problematic = []
        for rule_id, stats in issue_stats.items():
            agreement_rate = stats['agreed'] / stats['total']
            if agreement_rate < 0.5 and stats['total'] >= 10:  # At least 10 samples
                problematic.append({
                    'rule': rule_id,
                    'agreement_rate': agreement_rate,
                    'sample_size': stats['total']
                })
        
        return sorted(problematic, key=lambda x: x['agreement_rate'])


# Example usage
if __name__ == '__main__':
    # Import your writing assistant
    import sys
    sys.path.append('/home/claude')
    from writing_assistant import WritingAssistant
    
    # Initialize
    assistant = WritingAssistant()
    tester = AccuracyTester(assistant)
    
    # Run tests
    print("Running accuracy tests...")
    report = tester.run_all_tests()
    
    # Print report
    tester.print_report(report)
    
    # Compare with benchmarks
    benchmark = BenchmarkComparison()
    comparisons = benchmark.compare(report['by_category'])
    benchmark.print_comparison(comparisons)
    
    # Save detailed results
    with open('/home/claude/accuracy_report.json', 'w') as f:
        # Convert dataclasses to dicts for JSON serialization
        serializable_report = {
            'overall': report['overall'],
            'by_category': report['by_category'],
            'total_tests': report['total_tests'],
            'passed_tests': report['passed_tests']
        }
        json.dump(serializable_report, f, indent=2)
    
    print("\n\nDetailed results saved to accuracy_report.json")

